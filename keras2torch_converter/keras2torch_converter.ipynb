{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "In this workbook I have described my way of converting weights from keras to torch. In the beginning you will find some issues I encountered with and my way of solving them. Then you will find a code of converting. \n",
    "\n",
    "My motivation or how the story began:\n",
    "I have downloaded model with a pretrained model from here https://github.com/uzh-rpg/rpg_public_dronet. But I am torch user and wished to convert model from keras to torch. The original keras model could be find in *'keras_model.py'* and the weights in *'model_weights.h5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My path\n",
    "For some reasons, online available converter (https://github.com/Microsoft/MMdnn) didn't help me. It could convert model successfuly, but results of feeding the same data were different. So I decided to implement code of the model on Torch by bare hands and then convert weights from keras to torch. Torch copy of the original Keras model could be find in *torch_model.py*. My solution for converting weights was based on the mapping of parameters names between keras and torch. So here the task of mapping creating appeared. I couldn't find smart solution for it, so decided to make it by hands as well. I have extracted names of torch model \n",
    "> [(weight.name, weight.shape) for weight in keras_model.weights]\n",
    "\n",
    "and names of keras model like this:\n",
    "> [(key, torch_model.state_dict()[key].shape) for key in torch_model.state_dict().keys()]\n",
    "\n",
    "I have used shape just as a good hint for correct mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next problems\n",
    "\n",
    "After having such dictionary I encountered with some problems:\n",
    "\n",
    "Here: [w=width, h=height, i=input_channel_size, o=output_channel_size]\n",
    "\n",
    "1) **Convolution form issue.** Matrix of convolution in Keras has a form (w, h, i, o), while in torch (o, i, w, h)\n",
    "\n",
    "2) **Dense form issue.** Matrix of dense in Keras has a form (i,o) while linear in torch (o, i)\n",
    "\n",
    "3) **batch_norm issue.** Batch normalization has different default parameters (in particular momentum (0.1 in torch and 0.99 in keras) and eps (1e-5 in torch and 1e-3 in keras)\n",
    "\n",
    "4) **padding issue.** In keras there is a parameter of convolution which is called padding which could be 'same', 'valid' and etc. But this 'same' is very tricky it could add one column on the left side of a picture and two on the right. In torch padding in convolution is an integer, so it is impossible to set one column on the left and not one on the right. So for this purposes I have used nn.ZeroPad2d which takes tuple of four element one for left, one for right, one for down and one for up.\n",
    "\n",
    "5) **ReLU()** works with inplace=True by default. Which means, that one you run ReLU()(x), your x has been changed once. \n",
    "\n",
    "6) **Flatten issue.** Keras convolution inputs and outputs should be percieved so that channel dimension is the last one, i.e. (batch_size, w, h, channel_size), but in torch it is quite different: (batch_size, channel_size, w, h). So it creates different flattened vectors on torch and keras. \n",
    "\n",
    "You can find in this workbook or in .py files some comments like \"# related to ___ issue\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_MODEL_PATH=\"./model_weights.h5\"\n",
    "DICT_PATH=\"./keras_torch_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras, torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 21:27:42.165055 139965437531968 deprecation.py:506] From /home/artem/.local/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:210: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0728 21:27:42.524592 139965437531968 deprecation.py:506] From /home/artem/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "%run ./keras_model.py\n",
    "keras_model = resnet8(200,200,1,1)\n",
    "keras_model.load_weights(KERAS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./torch_model.py\n",
    "\n",
    "torch_model = ResNet8()\n",
    "torch_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.random.uniform(size=(200,200,1))[None]\n",
    "keras_img=img\n",
    "torch_img = torch.tensor(img.transpose((0, 3, 1, 2)), dtype=torch.float32, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def get_dict():\n",
    "    krs2trch = pd.read_csv(DICT_PATH)\n",
    "    length = len(krs2trch)\n",
    "    return {krs2trch.iloc[i].keras : krs2trch.iloc[i].torch for i in range(length)}\n",
    "\n",
    "\n",
    "def keras_to_pyt(km, pm):\n",
    "    name_mapping = get_dict()\n",
    "    weight_dict = dict()\n",
    "    for layer in km.layers:\n",
    "        weight_names = layer.weights\n",
    "        weight_values = layer.get_weights()\n",
    "        for i in range(len(weight_names)):\n",
    "            torch_name = name_mapping[weight_names[i].name]\n",
    "            if \"conv2d_\" in weight_names[i].name and \"kernel\" in weight_names[i].name:  # convolution from issue\n",
    "                weight_dict[torch_name] = np.transpose(weight_values[i], (3, 2, 0, 1))\n",
    "            elif \"dense_\" in weight_names[i].name and \"kernel\" in weight_names[i].name:  # dense from issue\n",
    "                weight_dict[torch_name] = np.transpose(weight_values[i], (1, 0))\n",
    "            else:\n",
    "                weight_dict[torch_name] = weight_values[i]\n",
    "                    \n",
    "    pyt_state_dict = pm.state_dict()\n",
    "    for key in weight_dict:\n",
    "        pyt_state_dict[key] = torch.from_numpy(weight_dict[key])\n",
    "    pm.load_state_dict(pyt_state_dict)\n",
    "\n",
    "keras_to_pyt(keras_model, torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -9.536743e-07)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_output=keras_model.predict_on_batch(img)\n",
    "torch_model.train(False)\n",
    "torch_output=torch_model(torch_img)\n",
    "#np.allclose(torch_output[0].detach().numpy().transpose(0,2,3,1), keras_output[0],atol=1e-5)\n",
    "np.allclose(torch_output[0].detach().cpu().numpy(), keras_output[0],atol=1e-5)\n",
    "#np.max(torch_output[0].detach().numpy().transpose(0,2,3,1) - keras_output[0])\n",
    "np.max(torch_output[1].detach().cpu().numpy() - keras_output[1]), np.max(torch_output[0].detach().cpu().numpy() - keras_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch_model.state_dict(), \"./torch_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
